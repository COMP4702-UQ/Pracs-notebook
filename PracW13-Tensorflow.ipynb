{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/COMP4702-UQ/Pracs-notebook/blob/main/PracW13-Tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# COMP4702/7703 - Machine Learning\n",
        "# Prac W13: Generative Models\n",
        "\n",
        "# Autoencoders\n",
        "\n",
        "The code in this exercise is based on the \"Introduction to autoencoders\" tutorial available at: https://www.tensorflow.org/tutorials/generative/autoencoder\n",
        "\n",
        "We start by building an autoencoder and training it on the fashion MNIST data."
      ],
      "metadata": {
        "id": "ZT_25pE_Pnzu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import layers, losses\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from tensorflow.keras.models import Model\n",
        "#---\n",
        "(x_train, _), (x_test, _) = fashion_mnist.load_data()\n",
        "\n",
        "x_train = x_train.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "\n",
        "print (x_train.shape)\n",
        "print (x_test.shape)\n",
        "#---\n",
        "class Autoencoder(Model):\n",
        "  def __init__(self, latent_dim, shape):\n",
        "    super(Autoencoder, self).__init__()\n",
        "    self.latent_dim = latent_dim\n",
        "    self.shape = shape\n",
        "    self.encoder = tf.keras.Sequential([\n",
        "      layers.Flatten(),\n",
        "      layers.Dense(latent_dim, activation='relu'),\n",
        "    ])\n",
        "    self.decoder = tf.keras.Sequential([\n",
        "      layers.Dense(tf.math.reduce_prod(shape).numpy(), activation='sigmoid'),\n",
        "      layers.Reshape(shape)\n",
        "    ])\n",
        "\n",
        "  def call(self, x):\n",
        "    encoded = self.encoder(x)\n",
        "    decoded = self.decoder(encoded)\n",
        "    return decoded\n",
        "\n",
        "\n",
        "shape = x_test.shape[1:]\n",
        "latent_dim = 64\n",
        "autoencoder = Autoencoder(latent_dim, shape)\n",
        "#---\n",
        "autoencoder.compile(optimizer='adam', loss=losses.MeanSquaredError())"
      ],
      "metadata": {
        "id": "y3GlASPHa5AX",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training\n",
        "#---\n",
        "autoencoder.fit(x_train, x_train,\n",
        "                epochs=10,\n",
        "                shuffle=True,\n",
        "                validation_data=(x_test, x_test))\n",
        "#---\n",
        "encoded_imgs = autoencoder.encoder(x_test).numpy()\n",
        "decoded_imgs = autoencoder.decoder(encoded_imgs).numpy()\n",
        "#---\n",
        "n = 10\n",
        "plt.figure(figsize=(20, 4))\n",
        "for i in range(n):\n",
        "  # display original\n",
        "  ax = plt.subplot(2, n, i + 1)\n",
        "  plt.imshow(x_test[i])\n",
        "  plt.title(\"original\")\n",
        "  plt.gray()\n",
        "  ax.get_xaxis().set_visible(False)\n",
        "  ax.get_yaxis().set_visible(False)\n",
        "\n",
        "  # display reconstruction\n",
        "  ax = plt.subplot(2, n, i + 1 + n)\n",
        "  plt.imshow(decoded_imgs[i])\n",
        "  plt.title(\"reconstructed\")\n",
        "  plt.gray()\n",
        "  ax.get_xaxis().set_visible(False)\n",
        "  ax.get_yaxis().set_visible(False)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pxPe0mF0Qrll",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I think this example uses a weird autoencoder - it has a single layer of 64 units, with ReLU activations. Note that if we use linear activation functions in the hidden layer, the network can perform the equivalent of PCA.\n",
        "\n",
        "**Q1**. Modify the network to use linear activation. Experiment to see if the performance is better/worse. Try training for longer and/or using stochastic gradient descent (sgd) as the optimizer.\n",
        "\n",
        "**Q2**. Experiment with a smaller/larger number of neurons in the hidden layer. Note that (apart from numerical issues) it should be possible to achieve 0 error if we have 784 hidden units (why?).\n",
        "\n",
        "**Q3**. Modify the network to be a proper nonlinear autoencoder. That is, you should have at least 3 hidden layers. It is not nescessary, but typically people make the structure symmetric - e.g. for this data: 784-hx-hy-hx-784. Experiment with training your network. It should be able to get a lower error, but it might take more training epochs.\n",
        "\n",
        "**Q4**. To use this network as a generative model, we can take the encoder part of the (trained) network, create \"fake\" input patterns for it, and see what output/image is reconstructed. Do this for one of your models from Q1-3 above. Experiment with different methods of creating fake input patterns.\n",
        "\n",
        "# Generative Adversarial Networks\n",
        "\n",
        "**Q5**. Read the first few pages of Chapter 15 of The Understanding Deep Learning book (https://udlbook.github.io/udlbook/). Then, work through Notebook 15.1 from that book.\n",
        "\n",
        "**Q6**. Read/work through this guide to building a GAN for the MNIST dataset:\n",
        "\n",
        "https://machinelearningmastery.com/how-to-develop-a-generative-adversarial-network-for-an-mnist-handwritten-digits-from-scratch-in-keras/\n"
      ],
      "metadata": {
        "id": "5CY-s0TFdi34"
      }
    }
  ]
}